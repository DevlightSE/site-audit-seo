Скрипт в `src/index.js` умеет парсить один или несколько сайтов в файлы csv в папке data.

Все детали зашиты в `src/scrap-site.js`.

## Особенности:
- Обходит весь сайт, собирает ссылки на страницы и документы
- Выгружает поля url и глубина, на которой найден url
- Документы с расширениями `doc`, `docx`, `xls`, `xlsx`, `pdf`, `rar`, `zip` добавляются в список с глубиной 0
- Поиск страниц с SSL mixed content
- Каждый сайт сохраняется в файл с именем домена
- Скрипт не ходит по ссылкам вне сканируемого домена
- Глубина по умолчанию 10
- По умолчанию сканирует в 2 потока
- Не загружает картинки, css, js (настраивается)
- Не считает урлы с ?width=123&height=123

## Установка:
```
git clone https://github.com/viasite/sites-scraper

cd sites-scraper
npm install
npm start
```

## Использование:
1. Открыть `data/sites.conf` (расширение выбрано исключительно ради подсветки и закомменчивания в vscode)
2. Вписать нужные сайты, по одному на строку (можно комментить ненужные сейчас через `#`, `//` или `;`)
3. Запустить: `npm start`

## Как посчитать контент по csv
1. Открыть в блокноте
2. Документы посчитать поиском `,0`
3. Листалки исключить поиском `?`
4. Вычесть 1 (шапка)

## Настройка
Основные настройки вынесены в `src/index.js`.

Главная настройка: `fields_preset`, определяет, какие колонки будут в CSV.

Все настройки делаются в `src/scrap-site.js`, комментарии там же.

## Баги
Иногда пишет в csv одинаковые страницы. Это бывает в 2 случаях: 
1. Редирект с другой страницы на эту (решается установкой `skipRequestedRedirect: true`, сделано).
2. Одновременный запрос одной и той же страницы в параллельных потоках.

## TODO:
- Не учитывать страницы ?page= , но сканировать
