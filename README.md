Скрипт в `src/index.js` умеет парсить один или несколько сайтов в файлы csv в папке data.

Все детали зашиты в `src/scrap-site.js`.

## Особенности:
- Обходит весь сайт, собирает ссылки на страницы и документы
- Выгружает поля url и глубина, на которой найден url
- Документы с расширениями `doc`, `docx`, `xls`, `xlsx`, `pdf`, `rar`, `zip` добавляются в список с глубиной 0
- Каждый сайт сохраняется в файл с именем домена
- Скрипт не ходит по ссылкам вне сканируемого домена
- Глубина по умолчанию 10
- По умолчанию сканирует в 2 потока
- Не загружает картинки
- Не считает урлы с ?width=123&height=123

## Установка:
- `git clone https://github.com/viasite/sites-scraper`
- `npm install`

## Использование:
1. Открыть src/index.js
2. Вписать нужные сайты
3. Запустить: `npm start'

## Как посчитать контент по csv
1. Открыть в блокноте
2. Документы посчитать поиском `,0`
3. Листалки исключить поиском `?`
4. Вычесть 1 (шапка)

## Настройка
Все настройки делаются в `src/scrap-site.js`, комментарии там же.

## Баги
- Иногда пишет в csv одинаковые страницы

## TODO:
- Не учитывать страницы ?page= , но сканировать
